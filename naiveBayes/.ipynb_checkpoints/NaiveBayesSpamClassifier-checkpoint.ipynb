{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes for Spam Classification\n",
    "\n",
    "\n",
    "Naive bayes is a relatively simple probabilistic classfication algorithm that is well suitable for categorical data .\n",
    "\n",
    "In machine learning, common application of Naive Bayes are spam email classification, sentiment analysis, document categorization. Naive bayes is advantageous over other commonly used classification algorithms in its simplicity, speed, and its accuracy on small data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "We will be using a data from the UCI machine learning repository that countains several Youtube comments from very popular music videos. Each comment in the data has been labeled as either spam or ham (legitimate comment), we will use this data to train our Naive Bayes algorithm for youtube comment spam classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# For matrix operations\n",
    "import numpy as np\n",
    "\n",
    "# For regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['content', 'label'], dtype='object')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from the 'YoutubeCommentsSpam.csv' file using pandas\n",
    "data_comments = pd.read_csv(\"YoutubeCommentsSpam.csv\")\n",
    "\n",
    "# Create column labels: 'content' and 'label'. \n",
    "# tips: the 'colums' method can be of help \n",
    "data_comments.rename(index=None, columns={'class':'label'}, inplace=True)\n",
    "\n",
    "# display the first rows of our dataset to make sure that the labels have been added\n",
    "# data_comments.head()\n",
    "data_comments.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{WARNING: DO NOT check the links in the spam comments! ;)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                +447935454150 lovely girl talk to me xxx\n",
      "2       my sister just received over 6,500 new <a rel=...\n",
      "4                               Hello I am from Palastine\n",
      "6       Go check out my rapping video called Four Whee...\n",
      "8                           Aslamu Lykum... From Pakistan\n",
      "10                            Help me get 50 subs please \n",
      "12      Alright ladies, if you like this song, then ch...\n",
      "15      <a href=\"https://www.facebook.com/groups/10087...\n",
      "16                  Take a look at this video on YouTube:\n",
      "17                 Check out our Channel for nice Beats!!\n",
      "19                    Check out this playlist on YouTube:\n",
      "21                                            like please\n",
      "24      I shared my first song &quot;I Want You&quot;,...\n",
      "25      Come and check out my music!Im spamming on loa...\n",
      "26                    Check out this playlist on YouTube:\n",
      "27      HUH HYUCK HYUCK IM SPECIAL WHO S WATCHING THIS...\n",
      "30      Check out this video on YouTube:<br /><br />Lo...\n",
      "33                    Check out this playlist on YouTube:\n",
      "34                       Check out this video on YouTube:\n",
      "35                       Check out this video on YouTube:\n",
      "38      Check out this playlist on YouTube:chcfcvzfzfb...\n",
      "39                   Check out this playlist on YouTube: \n",
      "40      Im gonna share a little ryhme canibus blows em...\n",
      "41                       Check out this video on YouTube:\n",
      "42      Check out this video on YouTube<br /><br /><br />\n",
      "43        CHECK OUT THE NEW REMIX !!!<br />CLICK CLICK !!\n",
      "44                    Check out this playlist on YouTube:\n",
      "45      I personally have never been in a abusive rela...\n",
      "48                                  plese subscribe to me\n",
      "49                       Check out this video on YouTube:\n",
      "                              ...                        \n",
      "1915             CHECK OUT partyman318 FR GOOD TUNEZ!! :D\n",
      "1916    Hey youtubers... I really appreciate all of yo...\n",
      "1917    Hey Music Fans I really appreciate any of you ...\n",
      "1918    Hey Music Fans I really appreciate any of you ...\n",
      "1919    Hey Music Fans I really appreciate any of you ...\n",
      "1920                   Hi. Check out and share our songs.\n",
      "1921                   Hi. Check out and share our songs.\n",
      "1922                    Hi.Check out and share our songs.\n",
      "1923    Hey Music Fans I really appreciate any of you ...\n",
      "1924    Hey, I am doing the Forty Hour famine so I ll ...\n",
      "1925             Love itt and ppl check out my channel!!!\n",
      "1926                                 SUBSCRIBE MY CHANNEL\n",
      "1927                                       adf.ly / KlD3Y\n",
      "1928                                       adf.ly / KlD3Y\n",
      "1929                               check out my new video\n",
      "1930    Hey Music Fans I really appreciate all of you ...\n",
      "1931    Hello everyone, It Is not my intention to spam...\n",
      "1932    ******* Facebook is LAME and so 2004! Check ou...\n",
      "1933    Please check out and send to others Freedom an...\n",
      "1934    Nice to meet You - this is Johnny: 1. If You a...\n",
      "1935     hey you ! check out the channel of Alvar Lake !!\n",
      "1936    Hi -this is Johnny: 1. If You already know my ...\n",
      "1940    Check out this video on YouTube:<br />&quot;Th...\n",
      "1942    O peoples of the earth, I have seen how you pe...\n",
      "1945    I WILL NEVER FORGET THIS SONG IN MY LIFE LIKE ...\n",
      "1946    ********OMG Facebook is OLD! Check out  ------...\n",
      "1947    Hey Music Fans I really appreciate all of you ...\n",
      "1948    **CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...\n",
      "1949    **CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...\n",
      "1950    **CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...\n",
      "Name: content, Length: 1004, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Show spam comments in data\n",
    "# DO NOT GO ON THE LINKS BELOW!!! seriously, they're spams... \n",
    "print(data_comments[\"content\"][data_comments[\"label\"] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Browsing over the comments that have been labeled as spam in this data, it seems like these comments are either unrelated to the video, or are some form of advertisement. The phrase \"check out\" seems to be very popular in this comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics and Data Cleaning\n",
    "\n",
    "The table below shows that this data set consist of $1959$ youtube comments, about $49\\%$ of them are legitimate comments and about $51\\%$ are spam. This high variation of classes in our data set will help us test our algorithms accuracy on the test data set. The average length of each comment is about $96$ characters, which is roughly about $15$ words on average per comment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1959.000000</td>\n",
       "      <td>1959.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.512506</td>\n",
       "      <td>15.651863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499971</td>\n",
       "      <td>21.696751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>213.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label       length\n",
       "count  1959.000000  1959.000000\n",
       "mean      0.512506    15.651863\n",
       "std       0.499971    21.696751\n",
       "min       0.000000     1.000000\n",
       "25%       0.000000     5.000000\n",
       "50%       1.000000     8.000000\n",
       "75%       1.000000    16.000000\n",
       "max       1.000000   213.000000"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add another column with corresponding comment length\n",
    "# tips: use map and lambda\n",
    "data_comments['length'] = data_comments['content'].str.count(re.compile(\" +|$\")) #\\w+ ??\n",
    "### FAIRE AVEC MAP ET LAMBDA \n",
    "###\n",
    "\n",
    "# Display summary statistics (mean, stdev, min, max)\n",
    "data_comments[[\"label\",\"length\"]].describe()\n",
    "# data_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of evaluation our Naive Bayes classification algorithm, we will split the data into a training and test set. The training set will be used to train the spam classification algorithm, and the test set will only be used to test its accuracy. In general the training set should be bigger than the test set and both have should be drawn from the same population (population in our case is youtube comments for music videos). We will randomly select $75\\%$ of the data as training, and $25\\%$ of the data for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>uniform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+447935454150 lovely girl talk to me xxx</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.020960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my sister just received over 6,500 new &lt;a rel=...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.447920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cool</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.120542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wow this video almost has a billion views! Did...</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.649550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Go check out my rapping video called Four Whee...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.140671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label  length   uniform\n",
       "0           +447935454150 lovely girl talk to me xxx      1       7  0.020960\n",
       "2  my sister just received over 6,500 new <a rel=...      1      23  0.447920\n",
       "3                                               Cool      0       1  0.120542\n",
       "5  Wow this video almost has a billion views! Did...      0      16  0.649550\n",
       "6  Go check out my rapping video called Four Whee...      1      11  0.140671"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's split data into training and test set (75% training, 25% test)\n",
    "\n",
    "# Set seed so we get same random allocation on each run of code\n",
    "np.random.seed(2017)\n",
    "\n",
    "# Add column vector 'uniform' of randomly generated numbers from 0 to 1 \n",
    "# tips: in numpy there is a method to draw sample from a uniform distribution\n",
    "data_comments[\"uniform\"] = np.random.uniform(0, 1, data_comments.shape[0])\n",
    "\n",
    "# As the number in our 'uniform' column is uniformly distributed, \n",
    "# about 75% of these numbers should be less than 0.75, let's grab those 75%\n",
    "data_comments_train = data_comments[data_comments[\"uniform\"] < 0.75]\n",
    "\n",
    "# same for the 25% of these numbers should that are greater than 0.75\n",
    "data_comments_test = data_comments[data_comments[\"uniform\"] >= 0.75]\n",
    "\n",
    "data_comments_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1485.000000\n",
       "mean        0.509764\n",
       "std         0.500073\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         1.000000\n",
       "75%         1.000000\n",
       "max         1.000000\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that training data have both spam and ham comments\n",
    "data_comments_train[\"label\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    474.000000\n",
       "mean       0.521097\n",
       "std        0.500083\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        1.000000\n",
       "75%        1.000000\n",
       "max        1.000000\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same for the test data \n",
    "data_comments_test[\"label\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the training and test data have a good mix spam and ham comments, so we are ready to move onto training the Naive Bayes classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in training data: 5778\n",
      "First 5 words in our unique set of words: \n",
      " ['emotions', 'personally', 'talking', 'awesoooome', 'about']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['wholehearted',\n",
       " 'emotions',\n",
       " 'personally',\n",
       " 'talking',\n",
       " 'awesoooome',\n",
       " 'about',\n",
       " 'elongate',\n",
       " 'hrefhttpwwwyoutubecomwatchvkq6zr6kcpj8ampt2m19s219a',\n",
       " '4s',\n",
       " 'admirable',\n",
       " 'contest',\n",
       " 'gabby',\n",
       " 'gofundmecomgrwmps',\n",
       " 'sheldon',\n",
       " 'hrefhttpsplusgooglecoms23giraffebruuhgiraffebruuha',\n",
       " 'hello',\n",
       " 'ever',\n",
       " 'banging',\n",
       " 'folly',\n",
       " 'planet',\n",
       " 'senses',\n",
       " 'subscribeeeeeeeeee',\n",
       " 'yearquot',\n",
       " 'ones',\n",
       " 'horrific',\n",
       " 'fox',\n",
       " 'roar',\n",
       " 'floooooooooooooooooooop',\n",
       " '500k',\n",
       " 'tryna',\n",
       " 'instrumental',\n",
       " 'please',\n",
       " 'space',\n",
       " 'im',\n",
       " 'taking',\n",
       " 'httpimage2youru480511340490',\n",
       " 'should',\n",
       " 'song',\n",
       " 'khalifa',\n",
       " 'british',\n",
       " 'god',\n",
       " 'journey',\n",
       " 'worked',\n",
       " 'chiptunes',\n",
       " 'picked',\n",
       " 'thing',\n",
       " 'tracks',\n",
       " 'perrys',\n",
       " 'cs',\n",
       " 'c',\n",
       " 'big',\n",
       " 'stream',\n",
       " 'teacher',\n",
       " 'signup',\n",
       " 'httpwwwsunfrogshirtscomsunglassworldhtml24398',\n",
       " 'hii',\n",
       " 'lasting',\n",
       " 'fan',\n",
       " 'arive',\n",
       " 'roasted',\n",
       " '',\n",
       " 'sec',\n",
       " 'wow',\n",
       " 'also',\n",
       " 'ig',\n",
       " 'too',\n",
       " 'hay',\n",
       " 'subscribe',\n",
       " 'me',\n",
       " 'lion',\n",
       " 'keyword',\n",
       " 'httpimage2youru480511340492',\n",
       " 'files',\n",
       " 'sm',\n",
       " 'views',\n",
       " 'f',\n",
       " 'usa',\n",
       " 'hahaa',\n",
       " 'me',\n",
       " 'check',\n",
       " 'sponsor',\n",
       " 'btw',\n",
       " 'canvas',\n",
       " 'posts',\n",
       " 'lovee',\n",
       " 'voice',\n",
       " 'treu',\n",
       " 'im',\n",
       " 'spam',\n",
       " 'country',\n",
       " 'experiments',\n",
       " 'm',\n",
       " 'bitch',\n",
       " 'upcoming',\n",
       " 'chhanel',\n",
       " 'independent',\n",
       " 'led',\n",
       " 'brought',\n",
       " 'oportunity',\n",
       " 'bunch',\n",
       " 'wavelength',\n",
       " 'amazing',\n",
       " 'it',\n",
       " 'buy',\n",
       " 'skills',\n",
       " 'hood',\n",
       " 'tigerlike',\n",
       " 'it',\n",
       " 'millions',\n",
       " 'ð',\n",
       " 'memories',\n",
       " 'cabelo',\n",
       " 'google',\n",
       " 'possible',\n",
       " 'httpwwwreverbnationcommsmarilynmiles',\n",
       " 'look',\n",
       " 'hermann',\n",
       " 'ohand',\n",
       " 'src',\n",
       " 'painting',\n",
       " 'professionally',\n",
       " 'visor',\n",
       " 'analyst',\n",
       " 'shakiralt333âââââââââââã33âââ',\n",
       " 'about',\n",
       " 'friend',\n",
       " 'several',\n",
       " 'industry',\n",
       " 'tits',\n",
       " 'were',\n",
       " 'amazed',\n",
       " 'tell',\n",
       " 'basically',\n",
       " 'behind',\n",
       " 'weâ',\n",
       " 'whistleblower',\n",
       " 'vena',\n",
       " 'advance',\n",
       " 'w',\n",
       " 'gt',\n",
       " 'were',\n",
       " 'macho',\n",
       " 'asian',\n",
       " 'bitly1bsefqe',\n",
       " 'httpswwwfacebookcomgorlingoltzsupport',\n",
       " 'compared',\n",
       " 'courtthanks',\n",
       " 'spot',\n",
       " '100',\n",
       " 'button',\n",
       " '1700000000',\n",
       " 'lovers',\n",
       " 'now',\n",
       " 'bestââ',\n",
       " 'swagfriends',\n",
       " 'succeeds',\n",
       " 'gets',\n",
       " '100100',\n",
       " 'life',\n",
       " 'thought',\n",
       " 'shoot',\n",
       " 'guitar',\n",
       " 'new',\n",
       " 'this',\n",
       " 'pe',\n",
       " 'some',\n",
       " 'anything',\n",
       " 'message',\n",
       " 'v',\n",
       " 'me',\n",
       " 'subscriber',\n",
       " 'likecomment',\n",
       " 'watchvarkglzjqup0',\n",
       " 'like',\n",
       " 'all',\n",
       " 'spraytan',\n",
       " 'extra',\n",
       " 'wwwyouniqueproductscomjoannagordon',\n",
       " 'ourself',\n",
       " 'monthly',\n",
       " 'wow',\n",
       " 'can',\n",
       " 'smoke',\n",
       " 'mockingbird',\n",
       " 'sones',\n",
       " 't',\n",
       " 'work',\n",
       " 'tsu',\n",
       " 'sam',\n",
       " 'gtgt',\n",
       " 'worried',\n",
       " 'month',\n",
       " 'monstrous',\n",
       " 'd90',\n",
       " 'dede',\n",
       " 'soooooooooooooooooooooooooooooooooooooooooooooooooo',\n",
       " 'misty',\n",
       " 'looking',\n",
       " 'freedom',\n",
       " 'took',\n",
       " 'video',\n",
       " 'follow',\n",
       " 'inspiring',\n",
       " 'heard',\n",
       " 'marketglory',\n",
       " 'httpswwwfacebookcompagesd8aad8add985d98ad984d8a7d8acd985d984d8a7d984d8a7d985d988d8b3d98ad982d989___music674732645945877',\n",
       " 'itâs',\n",
       " 'httpwwwebaycomitm131275322914sspagenamestrkmeselxitamp_trksidp3984m1555l2649',\n",
       " 'smile',\n",
       " 'youtubebr',\n",
       " 'acting',\n",
       " '11',\n",
       " 'organizations',\n",
       " 'invito',\n",
       " 'everðððbr',\n",
       " '200',\n",
       " 'fav',\n",
       " '14',\n",
       " 'purpy',\n",
       " 'myself',\n",
       " 'hello',\n",
       " 'brings',\n",
       " 'retards',\n",
       " 'ep',\n",
       " '0058',\n",
       " 'â',\n",
       " 'start',\n",
       " 'channnnnnelll',\n",
       " 'too',\n",
       " 'ððððððððððð',\n",
       " 'roaaaaarrrrrr',\n",
       " 'joanna',\n",
       " 'record',\n",
       " 'present',\n",
       " 'guys',\n",
       " 'lauren',\n",
       " 'stay',\n",
       " 'hallowsbr',\n",
       " 'adoult',\n",
       " 's',\n",
       " 'cock',\n",
       " 'email',\n",
       " 'paid',\n",
       " 'subscribe',\n",
       " 'clean',\n",
       " 'worry',\n",
       " 'rised',\n",
       " 'wallet',\n",
       " 'it',\n",
       " '6500',\n",
       " 'seriously',\n",
       " 'mice',\n",
       " 'sooooo',\n",
       " 'awesome',\n",
       " 'drews',\n",
       " 'portugal',\n",
       " '666002018',\n",
       " 'breath',\n",
       " 'putty',\n",
       " 'nick',\n",
       " 'subscribe',\n",
       " 'somebody',\n",
       " 'smart',\n",
       " 'hdtv',\n",
       " 'tears',\n",
       " 'expect',\n",
       " 'whats',\n",
       " 'using',\n",
       " 'understand',\n",
       " 'vocãª',\n",
       " 'adapt',\n",
       " 'cazzy',\n",
       " 'httpwwwguardaloorgbestoffunnycatsgattipazziedivertenti20135287100000415527985',\n",
       " 'task',\n",
       " 'doe',\n",
       " '2016',\n",
       " 'talent',\n",
       " 'dude',\n",
       " 'obrigado',\n",
       " '',\n",
       " 'em',\n",
       " 'world',\n",
       " 'wind',\n",
       " 'more',\n",
       " 'houronly',\n",
       " 'senators',\n",
       " 'burder',\n",
       " 'qiameth',\n",
       " 'goodbye',\n",
       " 'song',\n",
       " 'httpwwwtwitchtvzxlightsoutxz',\n",
       " 'spit',\n",
       " '2b',\n",
       " 'inspiration',\n",
       " 'duo',\n",
       " '800000000',\n",
       " 'found',\n",
       " 'song',\n",
       " '1billiom',\n",
       " 'acceptance',\n",
       " 'ambition',\n",
       " 'âbut',\n",
       " 'sharing',\n",
       " 'bars',\n",
       " 'profits',\n",
       " 'ladies',\n",
       " 'ma',\n",
       " 'swag',\n",
       " 'whatever',\n",
       " 'subscribed',\n",
       " 'elephant',\n",
       " '16',\n",
       " 'us',\n",
       " 'welcome',\n",
       " 'ok',\n",
       " 'me',\n",
       " 'guy',\n",
       " '100',\n",
       " 'birthday',\n",
       " 'potter',\n",
       " 'dinero',\n",
       " 'mississippi',\n",
       " 'whiz',\n",
       " 'youquot',\n",
       " 'ð¼ðð',\n",
       " 'watchvno9povz9oiqamplistuultucdihsdei01by1ow7wuq',\n",
       " 'famous',\n",
       " 'whole',\n",
       " 'mini',\n",
       " 'setting',\n",
       " 'ï¼ï¼ï¼ï¼',\n",
       " 'crash',\n",
       " 'httpsbinboxiodnckmqt4q1jb1',\n",
       " 'remix',\n",
       " 'comment',\n",
       " 'umph',\n",
       " 'way',\n",
       " 'normal',\n",
       " 'down',\n",
       " 'songs',\n",
       " 'funnytortspics',\n",
       " 'luck',\n",
       " 'released',\n",
       " 'hopme',\n",
       " 'hrefhttpwwwyoutubecomwatchvkq6zr6kcpj8ampt4m11s411a',\n",
       " 'ahhh',\n",
       " 'httpswwwchangeorgpfacebooktwitteryoutubedonotcensorjulienblanc',\n",
       " 'invest',\n",
       " 'store',\n",
       " 'aid',\n",
       " '1',\n",
       " 'tries',\n",
       " 'ends',\n",
       " 'also',\n",
       " 'lots',\n",
       " 'reading',\n",
       " 'respect',\n",
       " 'must',\n",
       " 'â',\n",
       " 'everything',\n",
       " 'thatâs',\n",
       " 'daughters',\n",
       " 'billie',\n",
       " 'wear',\n",
       " 'everlt3',\n",
       " 'fucking',\n",
       " 'type',\n",
       " 'watchvdtqcftr1fac',\n",
       " 'wait',\n",
       " 'phenomena',\n",
       " 'wtp',\n",
       " '23',\n",
       " 'spirit',\n",
       " 'he',\n",
       " '250',\n",
       " 'cheetos',\n",
       " 'kidsmediausa',\n",
       " 'rock',\n",
       " 'attention',\n",
       " 'impress',\n",
       " 'heartbreaking',\n",
       " 'lose',\n",
       " 'community',\n",
       " 'fantastic',\n",
       " 'sucscribe',\n",
       " 'httpspayhipcombotib',\n",
       " 'blog',\n",
       " 'mates',\n",
       " 'billon',\n",
       " 'sure',\n",
       " 'gave',\n",
       " 'training',\n",
       " 'right',\n",
       " 'it',\n",
       " 'irish',\n",
       " 'quotfeelin',\n",
       " 'real',\n",
       " '',\n",
       " 'jr',\n",
       " '40',\n",
       " 'fashionable',\n",
       " 'much',\n",
       " 'vistazo',\n",
       " 'take',\n",
       " 'views',\n",
       " 'haha',\n",
       " 'tiget',\n",
       " 'giant',\n",
       " 'fire',\n",
       " 'nite',\n",
       " 'dances',\n",
       " 'password',\n",
       " 'tazz',\n",
       " 'name',\n",
       " 'any',\n",
       " 'wooooo',\n",
       " 'xoxo',\n",
       " 'boobs',\n",
       " 'old',\n",
       " 'it',\n",
       " 'says',\n",
       " 'wwwswagbuckscomrefernonturtle02',\n",
       " 'ah',\n",
       " 'zesty',\n",
       " 'httpinstagramcompsmzdivopxb',\n",
       " 'warcraft',\n",
       " 'set',\n",
       " 'cking',\n",
       " 'ana',\n",
       " 'heshe',\n",
       " 'worthless',\n",
       " 'channelbr',\n",
       " 'boooobs',\n",
       " 'bout',\n",
       " 'shuffle',\n",
       " 'price510',\n",
       " 'senate',\n",
       " 'free',\n",
       " 'bonus',\n",
       " 'watchvvtarggvgtwq',\n",
       " 'funny',\n",
       " 'yourself',\n",
       " 'splashes',\n",
       " 'potential',\n",
       " 'incquot',\n",
       " 'enormously',\n",
       " 'money',\n",
       " '11th',\n",
       " 'vi',\n",
       " 'ââââââââââââââââââââââââââ',\n",
       " 'formula',\n",
       " 'distribution',\n",
       " 'edge',\n",
       " 'idol',\n",
       " 'et',\n",
       " 'plus',\n",
       " 'hot',\n",
       " 'foolish',\n",
       " 'nikon',\n",
       " 'freakin',\n",
       " 'song',\n",
       " 'install',\n",
       " 'louis',\n",
       " 'drop',\n",
       " 'sell',\n",
       " 'fausto',\n",
       " 'bet',\n",
       " 'lamest',\n",
       " 'nails',\n",
       " 'doesnt',\n",
       " 'megan',\n",
       " 'tuto',\n",
       " 'highly',\n",
       " 'ever',\n",
       " 'life',\n",
       " 'quiet',\n",
       " 'weird',\n",
       " 'koean',\n",
       " 'beutiful',\n",
       " 'âââââºâºmy',\n",
       " 'recently',\n",
       " 'ginius',\n",
       " 'reminiscent',\n",
       " 'watchvyuttx04oyqq',\n",
       " '4gb',\n",
       " '',\n",
       " 'beats',\n",
       " 'russia',\n",
       " 'per',\n",
       " 'acting',\n",
       " 'watching',\n",
       " 'gardner',\n",
       " 'shkira',\n",
       " 'minaj',\n",
       " 'covered',\n",
       " 'oil',\n",
       " 'first',\n",
       " 'channelthanks',\n",
       " 'day',\n",
       " 'made',\n",
       " 'belly',\n",
       " 'ass',\n",
       " 'decio',\n",
       " 'lies',\n",
       " 'otherwise',\n",
       " 'lmfaois',\n",
       " 'contest',\n",
       " 'everyone',\n",
       " 'anyone',\n",
       " 'kitty',\n",
       " 'fiddle',\n",
       " 'didnt',\n",
       " 'everyone',\n",
       " 'sprayon',\n",
       " 'end',\n",
       " 'donating',\n",
       " 'httppsnbosscomref2tggp3pv6l',\n",
       " 'eminem',\n",
       " 'wwwmarketglorycomstrategygamelordviperas',\n",
       " 'vanstone',\n",
       " 'minecraft',\n",
       " 'registr',\n",
       " 'muchââââ',\n",
       " 'httpswwwfacebookcommyfunnyriddles',\n",
       " 'renewal',\n",
       " 'incomeâonly',\n",
       " 'perpetrated',\n",
       " 'views',\n",
       " 'feedback',\n",
       " 'httpswwwfacebookcommarcossousa4frefts',\n",
       " 'but',\n",
       " 'party',\n",
       " 'charlie',\n",
       " 'ways',\n",
       " 'beats',\n",
       " 'love',\n",
       " 'wake',\n",
       " 'madly',\n",
       " 'ððððððððððððððððbr',\n",
       " 'fablife',\n",
       " 'understand',\n",
       " 'equals',\n",
       " 'cook',\n",
       " 'eyes',\n",
       " 'rules',\n",
       " 'support',\n",
       " 'âgangnam',\n",
       " 'fuck',\n",
       " 'cards',\n",
       " 'share',\n",
       " 'shirtplease',\n",
       " 'we',\n",
       " 'httpspremiumeasypromosappcomvoteme19924616375350',\n",
       " 'charity',\n",
       " 'train',\n",
       " 'fack',\n",
       " 'now',\n",
       " 'themselves',\n",
       " 'steel',\n",
       " 'cute',\n",
       " 'but',\n",
       " 'seeâ',\n",
       " 'feelz',\n",
       " 'far',\n",
       " 'likeeeeeeeee',\n",
       " 'short',\n",
       " 'ï¼ï¼ï¼ï¼y',\n",
       " 'raise',\n",
       " 'cents',\n",
       " 'com',\n",
       " 'reminds',\n",
       " 'looks',\n",
       " 'rihanna',\n",
       " 'âââââââbr',\n",
       " 'also',\n",
       " 'up',\n",
       " 'recipe',\n",
       " 'ððð',\n",
       " 'grade',\n",
       " 'tiger',\n",
       " 'subscribe',\n",
       " 'bf4',\n",
       " 'see',\n",
       " 'refers',\n",
       " 'thisthe',\n",
       " 'website',\n",
       " 'protect',\n",
       " 'quadrillion',\n",
       " 'aka',\n",
       " 'watchvteloa6rio8o',\n",
       " 'has',\n",
       " 'vet',\n",
       " 'simply',\n",
       " 'awesome',\n",
       " 'current',\n",
       " 'ricky',\n",
       " 'press',\n",
       " 'memorized',\n",
       " 'trash',\n",
       " 'ummm',\n",
       " 'trailer',\n",
       " 'copy',\n",
       " 'lt3',\n",
       " 'glasses',\n",
       " 'risk',\n",
       " 'giver',\n",
       " 'proud',\n",
       " 'votekatyperry',\n",
       " 'shame',\n",
       " 'wakad',\n",
       " 'so',\n",
       " 'against',\n",
       " 'favorbr',\n",
       " 'httpwwwamazoncoukgpofferlistingb00ecvf93gsr82qid1415297812refolp_tab_refurbishedieutf8ampconditionrefurbishedampqid1415297812ampsr82',\n",
       " 'again',\n",
       " 'america',\n",
       " 'sillyquot',\n",
       " 'plane',\n",
       " 'democracy',\n",
       " 'addicting',\n",
       " 'visits',\n",
       " 'epic',\n",
       " 'mum',\n",
       " 'witnesss',\n",
       " 'anywon',\n",
       " 'fucking',\n",
       " 'losing',\n",
       " 'aswell',\n",
       " 'hes',\n",
       " 'white',\n",
       " 'fighting',\n",
       " 'great',\n",
       " 'vid',\n",
       " 'lad',\n",
       " 'others',\n",
       " 'pie',\n",
       " 'lyrical',\n",
       " 'airlines',\n",
       " '',\n",
       " 'weekend',\n",
       " 'â',\n",
       " 'et',\n",
       " 'mile',\n",
       " 'br',\n",
       " 'month',\n",
       " 's',\n",
       " 'got',\n",
       " 'satisfying',\n",
       " 'my',\n",
       " 'job',\n",
       " 'possiblequot',\n",
       " 'piano',\n",
       " 'went',\n",
       " 'men',\n",
       " 'pleasant',\n",
       " 'lacked',\n",
       " 'willing',\n",
       " 'french',\n",
       " 'sing',\n",
       " 'line',\n",
       " 'desire',\n",
       " 'swoquix',\n",
       " 'httpswwwfacebookcomnicushorbboy',\n",
       " 'songsearch',\n",
       " 'doctor',\n",
       " 'aloidia',\n",
       " 'cheer',\n",
       " 'rymeluv',\n",
       " 'put',\n",
       " 'active',\n",
       " 'herehttpswwwfacebookcomtlouxmusic',\n",
       " 'ask',\n",
       " 'taylor',\n",
       " 'metal',\n",
       " 'briefs',\n",
       " 'realized',\n",
       " 'ââââââââââââââââbr',\n",
       " 'ampi',\n",
       " 'itâs',\n",
       " 'aslamu',\n",
       " '_',\n",
       " 'warning',\n",
       " 'yt',\n",
       " 'titled',\n",
       " 'guys',\n",
       " 'flight',\n",
       " 'devils',\n",
       " 'fashion',\n",
       " 'it',\n",
       " 'hi',\n",
       " 'radio',\n",
       " 'on',\n",
       " 'mountain',\n",
       " 'rewards',\n",
       " 'homies',\n",
       " 'saying',\n",
       " 'face',\n",
       " 'company',\n",
       " 'eminemlt3âbr',\n",
       " 'all',\n",
       " 'raps',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'pun',\n",
       " '5c',\n",
       " 'disabled',\n",
       " 'beautiful',\n",
       " 'tank',\n",
       " 'httpshhortcomarhupweh5ab',\n",
       " 'wear',\n",
       " 'leads',\n",
       " 'constitution',\n",
       " 'sign',\n",
       " 'death',\n",
       " 'please',\n",
       " 'moneygqcom',\n",
       " 'httpswwwfacebookcompageskomediburdagel775510675841486',\n",
       " '1hmvtxbr',\n",
       " 'helping',\n",
       " 'dragons',\n",
       " 'send',\n",
       " 'sure',\n",
       " 'delete',\n",
       " 'kidz',\n",
       " 'however',\n",
       " 'sins',\n",
       " 'stupid',\n",
       " 'strawberry',\n",
       " 'music',\n",
       " 'shit',\n",
       " 'while',\n",
       " 'enterthen',\n",
       " 'do',\n",
       " 'maylaysia',\n",
       " 'agreeable',\n",
       " 'higher',\n",
       " '2010',\n",
       " 'pure',\n",
       " 'dream',\n",
       " 'conhece',\n",
       " 'shit',\n",
       " 'viewsbr',\n",
       " 'plant',\n",
       " 'away',\n",
       " 'clubbr',\n",
       " 'earth',\n",
       " 'earthbr',\n",
       " 'changeable',\n",
       " 'quotsubscribequot',\n",
       " 'good',\n",
       " 'girls',\n",
       " 'champion',\n",
       " 'jaylan',\n",
       " 'bady',\n",
       " 'tiger',\n",
       " 'town',\n",
       " 'friend',\n",
       " 'goal',\n",
       " 'slappers',\n",
       " 'wtf',\n",
       " 'pop',\n",
       " 'facebook',\n",
       " 'haters',\n",
       " 'lol',\n",
       " 'views',\n",
       " 'express',\n",
       " 'or',\n",
       " 'gained',\n",
       " 'suscriba',\n",
       " 'por',\n",
       " 'loving',\n",
       " 'buchmair',\n",
       " 'boring',\n",
       " 'dislikesepic',\n",
       " 'katie',\n",
       " 'micheal',\n",
       " 'company',\n",
       " 'mil',\n",
       " 'dot',\n",
       " 'achieved',\n",
       " 'cute',\n",
       " 'enough',\n",
       " 'vote',\n",
       " 'vevo',\n",
       " 'mscalifornia95',\n",
       " 'edbr',\n",
       " 'taaeecom',\n",
       " 'thats',\n",
       " 'world',\n",
       " 'gain',\n",
       " 'sit',\n",
       " 'hack',\n",
       " 'everybody',\n",
       " 'how',\n",
       " 'glamour',\n",
       " 'oid104999962146104962510supermariologanaspanâ',\n",
       " 'reunion',\n",
       " 'world',\n",
       " 'basketball',\n",
       " 'see',\n",
       " 'shout',\n",
       " 'selfie',\n",
       " 'counts',\n",
       " 'plese',\n",
       " 'scientific',\n",
       " 'belle',\n",
       " 'composer',\n",
       " 'gamestop',\n",
       " 'called',\n",
       " 'news',\n",
       " 'divine',\n",
       " 'night',\n",
       " 'strategizes',\n",
       " 'pouring',\n",
       " 'forward',\n",
       " 'playerto',\n",
       " 'me',\n",
       " 'derives',\n",
       " 've',\n",
       " 'comic',\n",
       " 'started',\n",
       " 'here',\n",
       " 'everbr',\n",
       " 'intelligent',\n",
       " 'reads',\n",
       " 'hotter',\n",
       " 'bulgaria',\n",
       " 'platform',\n",
       " 'memorable',\n",
       " 'quotcruz',\n",
       " 'thanks',\n",
       " 'com',\n",
       " 'conscious',\n",
       " 'great',\n",
       " 'motivate',\n",
       " 'truly',\n",
       " 'eva',\n",
       " 'tapes',\n",
       " 'please',\n",
       " 'face',\n",
       " 'police',\n",
       " 'amount',\n",
       " 'expose',\n",
       " 'agree',\n",
       " 'nothingplease',\n",
       " '2004',\n",
       " 'flies',\n",
       " 'gun',\n",
       " 'bringing',\n",
       " '666',\n",
       " 'daneja',\n",
       " 'makes',\n",
       " '17yr',\n",
       " 'overplayed',\n",
       " 'seeing',\n",
       " 'httpsplaygooglecomstoreappsdetailsidcombutalabsphotoeditor',\n",
       " 'gs',\n",
       " 'meaning',\n",
       " 'this',\n",
       " 'ebay',\n",
       " 'sister',\n",
       " 'world',\n",
       " 'im',\n",
       " 'trafficking',\n",
       " 'jbs',\n",
       " 'httpwwwgofundmecomlittlebrother',\n",
       " 'tv',\n",
       " 'â1',\n",
       " 'ambition',\n",
       " 'ideas',\n",
       " 'calls',\n",
       " 'enjoyable',\n",
       " 'ladies',\n",
       " 'phone',\n",
       " 'musicbut',\n",
       " 'only',\n",
       " 'offical',\n",
       " 'weren',\n",
       " 'hahahahah',\n",
       " 'muzik',\n",
       " 'hard',\n",
       " 'choice',\n",
       " 'please',\n",
       " 'secret',\n",
       " 'unelind',\n",
       " 'strength',\n",
       " 'xxbb5tczhm39hvzd',\n",
       " 'years',\n",
       " 'swift',\n",
       " 'dumb',\n",
       " 'âââââ',\n",
       " 'writingrecordingmixingperforming',\n",
       " 'abusedmistreated',\n",
       " 'piss',\n",
       " 'barspart',\n",
       " 'httpthepiratebaysetorrent10626048theexpendables32014dvdscrleakedcleanxvidmp3rarbg',\n",
       " 'partying',\n",
       " 'illegal',\n",
       " 'zero',\n",
       " 'coverup',\n",
       " 'road',\n",
       " 'bennett',\n",
       " 'locobot',\n",
       " 'advertisementsbr',\n",
       " 'donate',\n",
       " 'designs',\n",
       " 'live',\n",
       " 'please',\n",
       " 'songbr',\n",
       " 'giraffebr',\n",
       " 'home',\n",
       " '8',\n",
       " 'career',\n",
       " 'birthday',\n",
       " 'providers',\n",
       " 'sertave',\n",
       " 'i5',\n",
       " 'humankind',\n",
       " '50',\n",
       " 'killtheclockhd',\n",
       " 'out',\n",
       " 'lmfao',\n",
       " 'subscribe',\n",
       " 'site',\n",
       " 'mtv',\n",
       " 'u',\n",
       " 'aye',\n",
       " 'fathers',\n",
       " 'enjoyâbr',\n",
       " '50',\n",
       " 'yust',\n",
       " 'gangsta',\n",
       " 'youtubebr',\n",
       " 'birthday',\n",
       " 'unspookedâbr',\n",
       " 'free',\n",
       " 'super',\n",
       " 'unlike',\n",
       " 'girl',\n",
       " 'se',\n",
       " 'wow',\n",
       " 'covers',\n",
       " 'island',\n",
       " 'sheep',\n",
       " 'fefefefefegelein',\n",
       " 'wonder',\n",
       " 'peaceful',\n",
       " 'âââââââââââââââââââââââââ',\n",
       " 'below',\n",
       " 'site',\n",
       " 'likeee',\n",
       " 'hard',\n",
       " 'would',\n",
       " 'achieve',\n",
       " 'luxuriant',\n",
       " 'commit',\n",
       " 'player',\n",
       " 'rock',\n",
       " 'rehabilitate',\n",
       " 'last',\n",
       " 'scared',\n",
       " 'grateful',\n",
       " '',\n",
       " 'at',\n",
       " '1015',\n",
       " 'being',\n",
       " 'il',\n",
       " 'meghan',\n",
       " 'mixtapecheck',\n",
       " 'plifal',\n",
       " 'pepelexa',\n",
       " 'decided',\n",
       " 'posted',\n",
       " 'lost',\n",
       " 'doesnt',\n",
       " 'hioffpo',\n",
       " '7k',\n",
       " 'will',\n",
       " 'cover',\n",
       " 'solve',\n",
       " 'songstill',\n",
       " 'operation',\n",
       " '60inch',\n",
       " 'chooses',\n",
       " 'pleasebr',\n",
       " 'beast',\n",
       " 'quotlike',\n",
       " 'chanell',\n",
       " 'hello',\n",
       " 'hrefhttpsplusgooglecoms23sharesharea',\n",
       " '2',\n",
       " 'do',\n",
       " 'comments',\n",
       " 'inaccurate',\n",
       " 'suffering',\n",
       " 'significantly',\n",
       " '',\n",
       " '3',\n",
       " 'performance',\n",
       " 'x',\n",
       " ...]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join all the comments into a big list\n",
    "# tips: list.join()\n",
    "training_list_words = \" \".join(data_comments['content'])\n",
    "training_list_words = training_list_words.lower()\n",
    "training_list_words = re.sub(r\" +\", \" \", training_list_words)\n",
    "training_list_words = training_list_words.split(\" \")\n",
    "\n",
    "# Convert to lower case and get unique set of words\n",
    "train_unique_words = set(training_list_words)\n",
    "train_unique_words = [re.sub(r\"\\W\", \"\", x) for x in train_unique_words]\n",
    "\n",
    "# Number of unique words in training \n",
    "vocab_size_train = len(train_unique_words)\n",
    "\n",
    "# Description of summarized comments in training data\n",
    "print('Unique words in training data: %s' % vocab_size_train)\n",
    "print('First 5 words in our unique set of words: \\n % s' % list(train_unique_words)[1:6])\n",
    "\n",
    "train_unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes for Spam Classification\n",
    "\n",
    "ok, so here's the deal:\n",
    "\n",
    "- first we are going to separate our training data into 2 subsets: train and test\n",
    "\n",
    "- then create several functions to check how many time each word apparear in spam and not spam comments, check the probability of each word appearing in spam/not spam\n",
    "\n",
    "- then the 2 most important function: train() and classify()\n",
    "\n",
    "- And finally check the accurracy of our predictions\n",
    "\n",
    "Let's code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary with comment words as \"keys\", and their label as \"value\"\n",
    "trainPositive = dict()\n",
    "trainNegative = dict()\n",
    "\n",
    "# Intiailize classes to zero\n",
    "positiveTotal = 0\n",
    "negativeTotal = 0\n",
    "\n",
    "# Initialize Prob. of to zero, but float ;) \n",
    "pSpam = 0.0\n",
    "pNotSpam = 0.0\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize dictionary of words and their labels   \n",
    "for word in train_unique_words:\n",
    "    \n",
    "    # Classify all words for now as ham (legitimate)\n",
    "    trainPositive[word] = 0\n",
    "    trainNegative[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count number of times word in comment appear in spam and ham comments\n",
    "def processComment(comment,label):\n",
    "    global positiveTotal\n",
    "    global negativeTotal\n",
    "    \n",
    "    # Split comments into words\n",
    "    comment = set(comment.split(\" \").lower())\n",
    "    comment = [re.sub(r\"\\W\", \"\", x) for x in comment]\n",
    "    \n",
    "    # Go over each word in comment\n",
    "    for word in comment :\n",
    "        # check if comment is not spam\n",
    "        if label == 1 :\n",
    "            # Increment number of times word appears in not spam comments\n",
    "             trainPositive[word] += 1\n",
    "         # spam comments\n",
    "        elif label == 0 :\n",
    "            # Increment number of times word appears in spam comments\n",
    "            trainNegative[word] += 1\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "   \n",
    "        \n",
    "            \n",
    "           \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Prob(word|spam) and Prob(word|ham)\n",
    "def conditionalWord(word,label):\n",
    "   \n",
    "    # Laplace smoothing parameter\n",
    "    # remider: to have access to a global variable inside a function \n",
    "    # you have to specify it using the word 'global'\n",
    "    \n",
    "    \n",
    "    # word in ham comment\n",
    "    if(label == 0):\n",
    "        # Compute Prob(word|ham)\n",
    "        \n",
    "    \n",
    "    # word in spam comment\n",
    "    else:\n",
    "        \n",
    "        # Compute Prob(word|ham)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Prob(spam|comment) or Prob(ham|comment)\n",
    "def conditionalComment(comment,label):\n",
    "    \n",
    "    # Initialize conditional probability\n",
    "    prob_label_comment = 1.0\n",
    "    \n",
    "    # Split comments into list of words\n",
    "    \n",
    "    \n",
    "    # Go through all words in comments\n",
    "    for ...\n",
    "        \n",
    "        # Compute value proportional to P(label|comment)\n",
    "        # Conditional indepdence is assumed here\n",
    "        \n",
    "    \n",
    "    return prob_label_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train naive bayes by computing several conditional probabilities in training data\n",
    "def train():\n",
    "    # reminder: we will need pSpam and pNotSpam here ;) \n",
    "\n",
    "\n",
    "    # Initiailize our variables: the total number of comment and the number of spam comments \n",
    "\n",
    "    \n",
    "    # Go over each comment in training data \n",
    "    for ...\n",
    "        \n",
    "       # check if comment is spam or not \n",
    "    \n",
    "       # increment the values depending if comment is spam or not\n",
    "        \n",
    "       # update dictionary of spam and not spam comments\n",
    "    \n",
    "    \n",
    "    # Compute prior probabilities, P(spam), P(ham)\n",
    "    pSpam = \n",
    "    pNotSpam = \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run naive bayes\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classify comment are spam or ham\n",
    "def classify(comment):\n",
    "    \n",
    "    # get global variables\n",
    "    \n",
    "    \n",
    "    # Compute value proportional to Pr(comment|ham)\n",
    "    isNegative = \n",
    "    \n",
    "    # Compute value proportional to Pr(comment|spam)\n",
    "    isPositive = \n",
    "    \n",
    "    # Output True = spam, False = ham depending of the 2 previously compute variables\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize spam prediction in test data\n",
    "prediction_test = []\n",
    "\n",
    "# Get prediction accuracy on test data\n",
    "for ...\n",
    "\n",
    "    # add classified comment to prediction_test list \n",
    "    \n",
    "\n",
    "# Check accuracy: \n",
    "# first the number of correct prediction \n",
    "correct_labels = \n",
    "# then the mean of correct predictions\n",
    "test_accuracy = \n",
    "\n",
    "#print prediction_test\n",
    "print(\"Proportion of comments classified correctly on test set: %s\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to write some comments to see whether they are classified as spam or ham. Recall the \"True\" is for spam comments, and \"False\" is for ham comments. \n",
    "Try your own !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spam\n",
    "classify(\"Guys check out my new chanell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spam\n",
    "classify(\"I have solved P vs. NP, check my video https://www.youtube.com/watch?v=dQw4w9WgXcQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ham\n",
    "classify(\"I liked the video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ham\n",
    "classify(\"Its great that this video has so many views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to go further...\n",
    "## Extending Bag of Words by Using TF-IDF\n",
    "\n",
    "So far we have been using the Bag of Words model to represent comments as vectors. The \"Bag of Words\" is a list of all unique words found in the training data, then each comment can be represented by a vector that contains the frequency of each unique word that appeared in the comment. For example if the training data contains the words $(hi, how, my, grade, are, you),$ then the text \"how are you you\" can be represented by $(0,1,0,0,1,2).$ The main reason we do this in our application is because comments can vary in length, but the length of all unique words stays fixed. \n",
    "\n",
    "In our context, the TF-IDF is a measure of how important a word is in a comment relative to all the words in our training data. For example if a word such as \"the\" appeared in most of the comments, the TF-IDF would be small as this word does not help us differentiate accross comments. Note that \"TF\" stands for \"Term Frequency\", and \"IDF\" stands for \"Inverse Document Frequency\". In particular, \"TF\" denoted by $tf(w,c)$ is the number of times the word $w$ appears in the given comment $c$. Whereas \"IDF\" is a measure of how much information a given word provides in differentiating comments. Specefically, $IDF$ is formulated as $idf(w, D) = log(\\frac{\\text{Number of comments in train data $D$}}{\\text{Number of comments containing the word $w$}}).$ To combine \"TF\" and \"IDF\" together, we simple take the product, hence $$TFIDF = tf(w,c) \\times idf(w, D) = (\\text{Number of times $w$ appears in comment $c$})\\times log(\\frac{\\text{Number of comments in train data $D$}}{\\text{Number of comments containing the word $w$}}).$$\n",
    "Now the $TF-IDF$ can be used to weight the vectors that result from the \"Bag of Words\" approach. For example, suppose a comment contains \"this\" 2 times, hence $tf = 2$. If we then had 1000 comments in our traininig data, and the word \"this\" appears in 100 comments, $idf = log(1000/100) = 2.$ Therefore in this example, the TF-IDF weight would be 2*2 = 4 for the word \"this\" appear twice in a particular comment. To incorprate TF-IDF into the naive bayes setting, we can compute $$Pr(word|spam) = \\frac{\\sum_{\\text{c is spam}}TFIDF(word,c,D)}{\\sum_{\\text{word in spam c}}\\sum_{\\text{c is spam}}TFIDF(word,c,D)+ \\text{Number of unique words in data}},$$ where $TFIDF(word,c,D) = TF(word,c) \\times IDF(word,data).$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute tfidf(word, comment, data)\n",
    "def TFIDF(comment, train):\n",
    "    \n",
    "    # Split comment into list of words\n",
    "    comment = \n",
    "    \n",
    "    # Initiailize tf-idf for given comment\n",
    "    tfidf_comment = \n",
    "    \n",
    "    # Initiailize number of comments containing a word\n",
    "    num_comment_word = 0\n",
    "    \n",
    "    # Intialize index for words in comment\n",
    "    word_index = 0\n",
    "    \n",
    "    # Go over all words in comment\n",
    "    for...\n",
    "        \n",
    "        # Compute term frequence (tf)\n",
    "        # Count frequency of word in comment\n",
    "        tf = \n",
    "        \n",
    "        # Find number of comments containing word\n",
    "        for ...\n",
    "            \n",
    "            # Increment word counter if word found in comment\n",
    "            if ...\n",
    "        \n",
    "        # Compute inverse document frequency (idf)\n",
    "        # log(Total number of comments/number of comments with word)\n",
    "        idf = \n",
    "        \n",
    "        # Update tf-idf weight for word\n",
    "        \n",
    "        \n",
    "        # Reset number of comments containing a word\n",
    "        \n",
    "        \n",
    "        # Move onto next word in comment\n",
    "        \n",
    "        \n",
    "    return tfidf_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TFIDF(\"Check out my new music video plz\",data_comments_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
